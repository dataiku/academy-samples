{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from dataikuapi.dss.ml import DSSPredictionMLTaskSettings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these constants by your own values\n",
    "EXPERIMENT_TRACKING_FOLDER_NAME = \"tracking\"\n",
    "EXPERIMENT_TRACKING_FOLDER_CONNECTION = \"dataiku-managed-storage\"\n",
    "EXPERIMENT_NAME = \"Custom-Modeling\"\n",
    "\n",
    "MLFLOW_CODE_ENV_NAME = \"mlflow\"\n",
    "SAVED_MODEL_NAME = \"custom-model\"\n",
    "DATASET_TRAINING = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "def now_str() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment tracking (scikit-learn)\n",
    "\n",
    "This notebook contains a simple example to showcase the new Experiment Tracking capabilities of Dataiku. It explains how to perform several runs with different parameters, select the best run and promote it as a Saved Model version in a Dataiku Flow. It leverages:\n",
    "* the scikit-learn package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training data\n",
    "\n",
    "Our training data lives in the `labeled` Dataset, let's load it in a pandas DataFrame and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-42480bd2-7fbe-4826-a76e-f081dbea3e03\" \n",
       "            onclick=\"_export_df('42480bd2-7fbe-4826-a76e-f081dbea3e03')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(typeof(IPython) === \"undefined\" || !IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"42480bd2-7fbe-4826-a76e-f081dbea3e03\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>price_first_item_purchased</th>\n",
       "      <th>gender</th>\n",
       "      <th>ip</th>\n",
       "      <th>ip_geopoint</th>\n",
       "      <th>ip_country_code</th>\n",
       "      <th>pages_visited</th>\n",
       "      <th>campaign</th>\n",
       "      <th>high_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008dd99a0</td>\n",
       "      <td>73</td>\n",
       "      <td>10.0</td>\n",
       "      <td>F</td>\n",
       "      <td>193.148.113.242</td>\n",
       "      <td>POINT(-3.684 40.4172)</td>\n",
       "      <td>ES</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00105d1128</td>\n",
       "      <td>35</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "      <td>150.197.87.28</td>\n",
       "      <td>POINT(126.9741 37.5112)</td>\n",
       "      <td>KR</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001097c31c</td>\n",
       "      <td>39</td>\n",
       "      <td>22.0</td>\n",
       "      <td>F</td>\n",
       "      <td>110.203.10.55</td>\n",
       "      <td>POINT(113.722 34.7732)</td>\n",
       "      <td>CN</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0022a1402b</td>\n",
       "      <td>34</td>\n",
       "      <td>22.0</td>\n",
       "      <td>F</td>\n",
       "      <td>171.81.35.224</td>\n",
       "      <td>POINT(113.722 34.7732)</td>\n",
       "      <td>CN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00230b1e80</td>\n",
       "      <td>75</td>\n",
       "      <td>15.5</td>\n",
       "      <td>M</td>\n",
       "      <td>157.96.97.63</td>\n",
       "      <td>POINT(-0.1224 51.4964)</td>\n",
       "      <td>GB</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  age  price_first_item_purchased gender               ip              ip_geopoint ip_country_code  pages_visited  campaign  high_value\n",
       "0  0008dd99a0   73                        10.0      F  193.148.113.242    POINT(-3.684 40.4172)              ES              5         1         0.0\n",
       "1  00105d1128   35                        28.0      M    150.197.87.28  POINT(126.9741 37.5112)              KR              6         0         0.0\n",
       "2  001097c31c   39                        22.0      F    110.203.10.55   POINT(113.722 34.7732)              CN              7         0         0.0\n",
       "3  0022a1402b   34                        22.0      F    171.81.35.224   POINT(113.722 34.7732)              CN              1         0         0.0\n",
       "4  00230b1e80   75                        15.5      M     157.96.97.63   POINT(-0.1224 51.4964)              GB              4         1         0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding to reset in case code block runs again after running later code\n",
    "os.environ.pop('MLFLOW_TRACKING_SERVER_CERT_PATH', None)\n",
    "os.environ.pop('MLFLOW_TRACKING_SERVER_CERT_PATH', None)\n",
    "os.environ.pop('MLFLOW_TRACKING_INSECURE_TLS', None)\n",
    "\n",
    "\n",
    "client = dataiku.api_client()\n",
    "project_key = dataiku.default_project_key()\n",
    "project = client.get_project(project_key)\n",
    "\n",
    "training_dataset = dataiku.Dataset(DATASET_TRAINING)\n",
    "df = training_dataset.get_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working on a *binary classification* problem here, which is to predict whether or not a given customer is high value. This outcome is reflected by the `high_value` column which can either take the \"0.0\" or \"1.0\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"high_value\"\n",
    "target = df[target_name]\n",
    "data = df.drop(columns=[target_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found experiment tracking folder tracking with id eONvV97v\n"
     ]
    }
   ],
   "source": [
    "# Get-or-create Managed Folder (WIP)\n",
    "project_folders = project.list_managed_folders()\n",
    "folder = None\n",
    "if len(project_folders) > 0:\n",
    "    for mf in project_folders:\n",
    "        if mf[\"name\"] == EXPERIMENT_TRACKING_FOLDER_NAME:\n",
    "            folder_id = mf[\"id\"]\n",
    "            print(f\"Found experiment tracking folder {EXPERIMENT_TRACKING_FOLDER_NAME} with id {mf['id']}\")\n",
    "            folder = project.get_managed_folder(odb_id=folder_id)\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    # -- If you reach this point, you didn't find the experiment tracking folder among the existing ones.\n",
    "    if not folder:\n",
    "        print(\"Experiment tracking folder not found. Creating it...\")\n",
    "        folder = project.create_managed_folder(EXPERIMENT_TRACKING_FOLDER_NAME,\n",
    "                                   connection_name=EXPERIMENT_TRACKING_FOLDER_CONNECTION)\n",
    "else:\n",
    "    print(\"No folder found in project. Creating one for experiment tracking...\")\n",
    "    # Write the creation of the mf code here.\n",
    "    folder = project.create_managed_folder(EXPERIMENT_TRACKING_FOLDER_NAME,\n",
    "                                       connection_name=EXPERIMENT_TRACKING_FOLDER_CONNECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the experiment\n",
    "\n",
    "To prepare the grounds for our experiments, we need to create a few handles and define which MLFlow experiment we'll collect our runs into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/14 19:31:59 INFO mlflow.tracking.fluent: Experiment with name 'Custom-Modeling' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Starting set experiment\n",
      "Sleeping for 30s to ensure experiment is fully created\n",
      "Sleep done\n",
      "Done getting remote client\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import requests\n",
    "logging.getLogger(requests.packages.urllib3.__package__).setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "def get_or_create_experiment():\n",
    "    try:\n",
    "        mlflow_extension = project.get_mlflow_extension()\n",
    "        mlflow_handle = project.setup_mlflow(managed_folder=folder)\n",
    "        experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "        print(experiment)\n",
    "    except Exception as e:\n",
    "        print(\"An exception occurred:\", str(e))\n",
    "        experiment = None\n",
    "    \n",
    "    if experiment is None:\n",
    "        mlflow_extension = project.get_mlflow_extension()\n",
    "        mlflow_handle = project.setup_mlflow(managed_folder=folder)\n",
    "        print(\"Starting set experiment\")\n",
    "        experiment = mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "        print(\"Sleeping for 30s to ensure experiment is fully created\")\n",
    "        time.sleep(30)\n",
    "        print(\"Sleep done\")\n",
    "    else:\n",
    "        mlflow_extension = project.get_mlflow_extension()\n",
    "        mlflow_handle = project.setup_mlflow(managed_folder=folder)\n",
    "        experiment = mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "      \n",
    "    return experiment\n",
    "\n",
    "experiment = get_or_create_experiment()\n",
    "\n",
    "# Set remote configuration\n",
    "space_url = dataiku.get_custom_variables()[\"SPACE_URL\"]\n",
    "api_key = dataiku.get_custom_variables()[\"API_KEY\"]\n",
    "dataiku.set_remote_dss(space_url, api_key, no_check_certificate=True)\n",
    "dataiku.set_default_project_key(project_key)\n",
    "client = dataiku.api_client()\n",
    "project = client.get_project(project_key)\n",
    "print(\"Done getting remote client\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting\n",
    "\n",
    "The goal of experiment tracking is to *instrument the iterative process of ML model training* by collecting all parameters and results of each trial. To be more specific, within an **experiment**, you perform multiple **runs**, each run being different from the others because of the **parameters** you use for it. You also need to specific which **metrics** to track, they will reflect the performance of the model for a given set of parameters.\n",
    "\n",
    "In this notebook example, if you want to produce experiment runs:\n",
    "* edit the parameters in the 3.1 cell and run it\n",
    "* run the 3.2 cell to effectively... perform the run ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the parameters of our run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to log:\n",
      " {'categorical_cols': ['gender', 'ip_country_code'], 'numerical_cols': ['age', 'price_first_item_purchased', 'pages_visited', 'campaign'], 'model_algo': 'GradientBoostingClassifier', 'n_estimators': 300, 'loss': 'exponential', 'learning_rate': 0.1, 'max_depth': 3, 'random_state': 42, 'n_cv_folds': 5}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Metrics to log:\n",
      " ['f1_macro', 'roc_auc']\n"
     ]
    }
   ],
   "source": [
    "# Create run name\n",
    "run_params = {}\n",
    "run_metrics = {}\n",
    "\n",
    "# Define run parameters\n",
    "# -- Which columns to retain ?\n",
    "categorical_cols = [\"gender\", \"ip_country_code\"]\n",
    "run_params[\"categorical_cols\"] = categorical_cols\n",
    "numerical_cols = [\"age\", \"price_first_item_purchased\", \"pages_visited\", \"campaign\"]\n",
    "run_params[\"numerical_cols\"] = numerical_cols\n",
    "\n",
    "# --Which algorithm to use? Which hyperparameters for this algo to try?\n",
    "# ---Example: Gradient Boosting\n",
    "hparams = {\"n_estimators\": 300,\n",
    "          \"loss\": \"exponential\",\n",
    "          \"learning_rate\": 0.1,\n",
    "          \"max_depth\": 3,\n",
    "          \"random_state\": 42}\n",
    "clf = GradientBoostingClassifier(**hparams)\n",
    "model_algo = type(clf).__name__\n",
    "run_params[\"model_algo\"] = model_algo\n",
    "for hp in hparams.keys():\n",
    "    run_params[hp] = hparams[hp]\n",
    "\n",
    "# --Which cross-validation settings to use?\n",
    "n_cv_folds = 5\n",
    "cv = StratifiedKFold(n_splits=n_cv_folds)\n",
    "run_params[\"n_cv_folds\"] = n_cv_folds\n",
    "metrics = [\"f1_macro\", \"roc_auc\"]\n",
    "\n",
    "# --Let's print all of that to get a recap:\n",
    "print(f\"Parameters to log:\\n {run_params}\")\n",
    "print(100*'-')\n",
    "print(f\"Metrics to log:\\n {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the run and logging parameters, metrics and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run run-20240514193231 (id: run_20240514193231)...\n",
      "Running cross-validation...\n",
      "pipeline fit\n",
      "log parameters, metrics and model\n",
      "log params\n",
      "log metrics\n",
      "artifact path complete:  GradientBoostingClassifier-run_20240514193231\n",
      "<Experiment: artifact_location='dss-managed-folder://eONvV97v/custom_modeling_cPa', creation_time=1715715119822, experiment_id='custom_modeling_cPa', last_update_time=1715715119822, lifecycle_stage='active', name='Custom-Modeling', tags={}>\n",
      "Starting set experiment with remote client\n",
      "Done set experiment\n",
      "facilitate run promotion\n",
      "DONE! Your artifacts are available at dss-managed-folder://eONvV97v/custom_modeling_cPa/run_20240514193231/artifacts\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "dataiku.set_remote_dss((dataiku.get_custom_variables()[\"SPACE_URL\"]), (dataiku.get_custom_variables()[\"API_KEY\"]), no_check_certificate=True)\n",
    "dataiku.set_default_project_key(project_key)\n",
    "client = dataiku.api_client()\n",
    "client._session.verify = False\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "#call setup experiment again\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ.pop('MLFLOW_TRACKING_SERVER_CERT_PATH', None)\n",
    "os.environ.pop('MLFLOW_TRACKING_CLIENT_CERT_PATH', None)\n",
    "os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\"\n",
    "\n",
    "run_ts = now_str()\n",
    "run_name = f\"run-{run_ts}\"\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Starting run {run_name} (id: {run_id})...\")\n",
    "    # --Preprocessing\n",
    "    categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('categorical', categorical_preprocessor, categorical_cols),\n",
    "        ('numerical', 'passthrough', numerical_cols)])\n",
    "    \n",
    "    # --Pipeline definition (preprocessing + model)\n",
    "    pipeline = make_pipeline(preprocessor, clf)\n",
    "    \n",
    "    # --Cross-validation\n",
    "    print(f\"Running cross-validation...\")\n",
    "    scores = cross_validate(pipeline, data, target, cv=cv, scoring=metrics)\n",
    "    for m in [f\"test_{mname}\" for mname in metrics]:\n",
    "        run_metrics[f\"mean_{m}\"] = scores[m].mean()\n",
    "        run_metrics[f\"std_{m}\"] = scores[m].std()\n",
    "        \n",
    "    # --Pipeline fit\n",
    "    print(\"pipeline fit\")\n",
    "    pipeline.fit(X=data, y=target)\n",
    "    # --Log the order of the class label\n",
    "    run_params[\"class_labels\"] = [str(c) for c in pipeline.classes_.tolist()]\n",
    "    \n",
    "    # --Log parameters, metrics and model\n",
    "    print(\"log parameters, metrics and model\")\n",
    "    mlflow.log_params(params=run_params)\n",
    "    print(\"log params\")\n",
    "    mlflow.log_metrics(metrics=run_metrics)\n",
    "    print(\"log metrics\")\n",
    "    artifact_path = f\"{model_algo}-{run_id}\"\n",
    "    print(\"artifact path complete: \", artifact_path)\n",
    "    \n",
    "    with mlflow.start_run(run_id=run_id, nested=True):\n",
    "            artifact_path = f\"{model_algo}-{run_id}\"\n",
    "            mlflow.sklearn.log_model(sk_model=pipeline, artifact_path=artifact_path)\n",
    "    \n",
    "    # Set up MLflow\n",
    "    mlflow_extension = project.get_mlflow_extension()\n",
    "    mlflow_handle = project.setup_mlflow(managed_folder=folder)\n",
    "    # Get or create the experiment with the remote client\n",
    "    experiment = get_or_create_experiment()\n",
    "    print(\"Starting set experiment with remote client\")\n",
    "    experiment = mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "    print(\"Done set experiment\")\n",
    "#     mlflow.sklearn.log_model(sk_model=pipeline, artifact_path=artifact_path)\n",
    "    # --Set useful information to faciliate run promotion\n",
    "    print(\"facilitate run promotion\")\n",
    "    mlflow_extension.set_run_inference_info(run_id=run_id,\n",
    "                                            prediction_type=\"BINARY_CLASSIFICATION\",\n",
    "                                            classes=run_params[\"class_labels\"],\n",
    "                                            code_env_name=MLFLOW_CODE_ENV_NAME,\n",
    "                                            target=\"high_value\")\n",
    "    print(f\"DONE! Your artifacts are available at {run.info.artifact_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "analyzedDataset": "train",
  "createdOn": 1666967089230,
  "creator": "tyfrec.test@yahoo.com",
  "customFields": {},
  "dkuGit": {
   "gitReference": {
    "checkout": "main",
    "isDirty": false,
    "lastHash": "231db0a9f3535aff029828ba4717414b983dc143",
    "lastTimestamp": 1707831844000,
    "remote": "https://github.com/dataiku/academy-samples",
    "remotePath": "se-coder-workshop/notebooks/Custom Modeling.ipynb"
   },
   "lastInteraction": 1715703268417
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env mlflow)",
   "language": "python",
   "name": "py-dku-venv-mlflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "modifiedBy": "kateryna.savchyn@dataiku.com",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
