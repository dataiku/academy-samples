{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from dataikuapi.dss.ml import DSSPredictionMLTaskSettings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress SSL warnings\n",
    "logging.getLogger(requests.packages.urllib3.__package__).setLevel(logging.ERROR)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these constants by your own values\n",
    "EXPERIMENT_TRACKING_FOLDER_NAME = \"tracking\"\n",
    "EXPERIMENT_TRACKING_FOLDER_CONNECTION = \"dataiku-managed-storage\"\n",
    "EXPERIMENT_NAME = \"custom-modeling_v000\"\n",
    "\n",
    "MLFLOW_CODE_ENV_NAME = \"mlflow\"\n",
    "SAVED_MODEL_NAME = \"custom-model\"\n",
    "DATASET_TRAINING = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "def now_str() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Initialize MLflow connection securely\n",
    "def setup_mlflow_environment():\n",
    "    \"\"\"Setup MLflow environment variables for secure connections\"\"\"\n",
    "    # Clear any existing MLflow environment variables to avoid conflicts\n",
    "    os.environ.pop('MLFLOW_TRACKING_SERVER_CERT_PATH', None)\n",
    "    os.environ.pop('MLFLOW_TRACKING_CLIENT_CERT_PATH', None)\n",
    "    os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"True\"\n",
    "    # End any active MLflow runs to avoid conflicts\n",
    "    try:\n",
    "        mlflow.end_run()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment tracking (scikit-learn)\n",
    "\n",
    "This notebook contains a simple example to showcase the new Experiment Tracking capabilities of Dataiku. It explains how to perform several runs with different parameters, select the best run and promote it as a Saved Model version in a Dataiku Flow. It leverages:\n",
    "* the scikit-learn package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training data\n",
    "\n",
    "Our training data lives in the `labeled` Dataset, let's load it in a pandas DataFrame and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "setup_mlflow_environment()\n",
    "\n",
    "# Connect to Dataiku\n",
    "client = dataiku.api_client()\n",
    "client._session.verify = False  # Disable SSL verification consistently\n",
    "project_key = dataiku.default_project_key()\n",
    "project = client.get_project(project_key)\n",
    "\n",
    "training_dataset = dataiku.Dataset(DATASET_TRAINING)\n",
    "df = training_dataset.get_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working on a *binary classification* problem here, which is to predict whether or not a given customer is high value. This outcome is reflected by the `high_value` column which can either take the \"0.0\" or \"1.0\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"high_value\"\n",
    "target = df[target_name]\n",
    "data = df.drop(columns=[target_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get-or-create Managed Folder\n",
    "def get_or_create_managed_folder():\n",
    "    \"\"\"Get or create the managed folder for experiment tracking\"\"\"\n",
    "    project_folders = project.list_managed_folders()\n",
    "    folder = None\n",
    "    \n",
    "    # Look for existing folder\n",
    "    if len(project_folders) > 0:\n",
    "        for mf in project_folders:\n",
    "            if mf[\"name\"] == EXPERIMENT_TRACKING_FOLDER_NAME:\n",
    "                folder_id = mf[\"id\"]\n",
    "                print(f\"Found experiment tracking folder {EXPERIMENT_TRACKING_FOLDER_NAME} with id {mf['id']}\")\n",
    "                folder = project.get_managed_folder(odb_id=folder_id)\n",
    "                break\n",
    "    \n",
    "    # Create folder if not found\n",
    "    if not folder:\n",
    "        print(f\"Creating experiment tracking folder {EXPERIMENT_TRACKING_FOLDER_NAME}...\")\n",
    "        folder = project.create_managed_folder(\n",
    "            EXPERIMENT_TRACKING_FOLDER_NAME,\n",
    "            connection_name=EXPERIMENT_TRACKING_FOLDER_CONNECTION\n",
    "        )\n",
    "    \n",
    "    return folder\n",
    "\n",
    "# Get or create the managed folder\n",
    "folder = get_or_create_managed_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the experiment\n",
    "\n",
    "To prepare the grounds for our experiments, we need to create a few handles and define which MLFlow experiment we'll collect our runs into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment():\n",
    "    \"\"\"Get or create an MLflow experiment, avoiding redundant calls\"\"\"\n",
    "    mlflow_extension = project.get_mlflow_extension()\n",
    "    mlflow_handle = project.setup_mlflow(managed_folder=folder)\n",
    "    \n",
    "    try:\n",
    "        # Try to get existing experiment\n",
    "        experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "        if experiment is None:\n",
    "            print(f\"Creating new experiment: {EXPERIMENT_NAME}\")\n",
    "            experiment = mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "            # Give the system a moment to fully create the experiment\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(f\"Using existing experiment: {EXPERIMENT_NAME}\")\n",
    "            experiment = mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting/creating experiment: {str(e)}\")\n",
    "        # Retry once with a clean setup\n",
    "        setup_mlflow_environment()\n",
    "        mlflow_handle = project.setup_mlflow(managed_folder=folder)\n",
    "        experiment = mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "    \n",
    "    return experiment\n",
    "\n",
    "# Initialize MLflow once\n",
    "setup_mlflow_environment()\n",
    "mlflow_extension = project.get_mlflow_extension()\n",
    "mlflow_handle = project.setup_mlflow(managed_folder=folder)\n",
    "experiment = get_or_create_experiment()\n",
    "print(\"MLflow experiment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting\n",
    "\n",
    "The goal of experiment tracking is to *instrument the iterative process of ML model training* by collecting all parameters and results of each trial. To be more specific, within an **experiment**, you perform multiple **runs**, each run being different from the others because of the **parameters** you use for it. You also need to specific which **metrics** to track, they will reflect the performance of the model for a given set of parameters.\n",
    "\n",
    "In this notebook example, if you want to produce experiment runs:\n",
    "* edit the parameters in the 3.1 cell and run it\n",
    "* run the 3.2 cell to effectively... perform the run ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the parameters of our run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create run name\n",
    "run_params = {}\n",
    "run_metrics = {}\n",
    "\n",
    "# Define run parameters\n",
    "# -- Which columns to retain ?\n",
    "categorical_cols = [\"gender\", \"ip_country_code\"]\n",
    "run_params[\"categorical_cols\"] = categorical_cols\n",
    "numerical_cols = [\"age\", \"price_first_item_purchased\", \"pages_visited\", \"campaign\"]\n",
    "run_params[\"numerical_cols\"] = numerical_cols\n",
    "\n",
    "# --Which algorithm to use? Which hyperparameters for this algo to try?\n",
    "# ---Example: Gradient Boosting\n",
    "hparams = {\"n_estimators\": 300,\n",
    "          \"loss\": \"exponential\",\n",
    "          \"learning_rate\": 0.1,\n",
    "          \"max_depth\": 3,\n",
    "          \"random_state\": 42}\n",
    "clf = GradientBoostingClassifier(**hparams)\n",
    "model_algo = type(clf).__name__\n",
    "run_params[\"model_algo\"] = model_algo\n",
    "for hp in hparams.keys():\n",
    "    run_params[hp] = hparams[hp]\n",
    "\n",
    "# --Which cross-validation settings to use?\n",
    "n_cv_folds = 5\n",
    "cv = StratifiedKFold(n_splits=n_cv_folds)\n",
    "run_params[\"n_cv_folds\"] = n_cv_folds\n",
    "metrics = [\"f1_macro\", \"roc_auc\"]\n",
    "\n",
    "# --Let's print all of that to get a recap:\n",
    "print(f\"Parameters to log:\\n {run_params}\")\n",
    "print(100*'-')\n",
    "print(f\"Metrics to log:\\n {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the run and logging parameters, metrics and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have a clean MLflow environment\n",
    "setup_mlflow_environment()\n",
    "\n",
    "# Start the MLflow run\n",
    "run_ts = now_str()\n",
    "run_name = f\"run-{run_ts}\"\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Starting run {run_name} (id: {run_id})...\")\n",
    "    \n",
    "    # --Preprocessing\n",
    "    categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('categorical', categorical_preprocessor, categorical_cols),\n",
    "        ('numerical', 'passthrough', numerical_cols)])\n",
    "\n",
    "    # --Pipeline definition (preprocessing + model)\n",
    "    pipeline = make_pipeline(preprocessor, clf)\n",
    "\n",
    "    # --Cross-validation\n",
    "    print(f\"Running cross-validation...\")\n",
    "    scores = cross_validate(pipeline, data, target, cv=cv, scoring=metrics)\n",
    "    for m in [f\"test_{mname}\" for mname in metrics]:\n",
    "        run_metrics[f\"mean_{m}\"] = scores[m].mean()\n",
    "        run_metrics[f\"std_{m}\"] = scores[m].std()\n",
    "\n",
    "    # --Pipeline fit\n",
    "    print(\"Fitting pipeline...\")\n",
    "    pipeline.fit(X=data, y=target)\n",
    "    # --Log the order of the class label\n",
    "    run_params[\"class_labels\"] = [str(c) for c in pipeline.classes_.tolist()]\n",
    "\n",
    "    # --Log parameters, metrics and model\n",
    "    print(\"Logging parameters and metrics...\")\n",
    "    mlflow.log_params(params=run_params)\n",
    "    mlflow.log_metrics(metrics=run_metrics)\n",
    "    \n",
    "    # Define artifact path\n",
    "    artifact_path = f\"{model_algo}-{run_id}\"\n",
    "    \n",
    "    # Log the model\n",
    "    print(\"Logging model...\")\n",
    "    mlflow.sklearn.log_model(sk_model=pipeline, artifact_path=artifact_path)\n",
    "    \n",
    "    # Set useful information to facilitate run promotion\n",
    "    print(\"Setting run inference info...\")\n",
    "    try:\n",
    "        mlflow_extension.set_run_inference_info(\n",
    "            run_id=run_id,\n",
    "            prediction_type=\"BINARY_CLASSIFICATION\",\n",
    "            classes=run_params[\"class_labels\"],\n",
    "            code_env_name=MLFLOW_CODE_ENV_NAME,\n",
    "            target=\"high_value\"\n",
    "        )\n",
    "        print(f\"DONE! Your artifacts are available at {run.info.artifact_uri}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not set run inference info: {str(e)}\")\n",
    "        print(f\"Run completed, but inference info could not be set. Artifacts are available at {run.info.artifact_uri}\")"
   ]
  }
 ],
 "metadata": {
  "analyzedDataset": "train",
  "createdOn": 1666967089230,
  "creator": "tyfrec.test@yahoo.com",
  "customFields": {},
  "dkuGit": {
   "gitReference": {
    "checkout": "main",
    "isDirty": false,
    "lastHash": "337fcf1ac686c02144fbd814156e51c584f744c6",
    "lastTimestamp": 1686842212000,
    "remote": "https://github.com/kirstenhoogenakker/coderhandson.git",
    "remotePath": "Custom Modeling.ipynb"
   },
   "lastInteraction": 1687283675621
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.9.18 64-bit ('3.9.18')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "modifiedBy": "alexandru.tautu@dataiku.com",
  "tags": [],
  "vscode": {
   "interpreter": {
    "hash": "ada874fbc53bf57f272b5137fa5cfef71c456165b41f2a33ec512f26bd5de78b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
